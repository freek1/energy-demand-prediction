{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "e0ark8SXuFPw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn import svm, ensemble\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "weather = pd.read_csv('data/weather.csv')\n",
        "pallet_history = pd.read_csv('data/Pallet_history_Gold_Spike.csv')\n",
        "inbound = pd.read_csv('data/inbound_loads.csv')\n",
        "outbound = pd.read_csv('data/outbound_laods.csv')\n",
        "demand = pd.read_csv('data/demand_kWtrain_val.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing the csv files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " #####\n",
            "Inbound: \n",
            " ##### \n",
            "\n",
            "Index(['front_temperature', 'middle_temperature', 'back_temperature',\n",
            "       'net_weight', 'case_quantity', 'pallet_count', 'truck_signin_datetime',\n",
            "       'load_time', 'truck_time'],\n",
            "      dtype='object')\n",
            "With NaN: (56146, 9)\n",
            "Without NaN: (8761, 9)\n",
            "\n",
            " #####\n",
            "Outbound: \n",
            " ##### \n",
            "\n",
            "Index(['net_weight', 'case_quantity', 'pallet_count', 'truck_signin_datetime',\n",
            "       'load_time', 'truck_time'],\n",
            "      dtype='object')\n",
            "With NaN: (112363, 6)\n",
            "Without NaN: (96704, 6)\n",
            "\n",
            " #####\n",
            "Demand:  \n",
            " ##### \n",
            "\n",
            "Full dataset: (365349, 3)\n",
            "Answers known until index:  273987\n",
            "Training set, 70%: (191789, 3)\n",
            "Validation set, 30%: (82197, 3)\n",
            "\n",
            " #####\n",
            "Weather:  \n",
            " ##### \n",
            "\n",
            "   Unnamed: 0  Relative Humidity  Temperature  hour    datetime_america\n",
            "0           0              50.37         53.6    18 2018-12-31 18:00:00\n",
            "1           1              50.37         53.6    18 2018-12-31 18:05:00\n",
            "2           2              50.37         53.6    18 2018-12-31 18:10:00\n",
            "3           3              50.37         53.6    18 2018-12-31 18:15:00\n",
            "4           4              50.37         53.6    18 2018-12-31 18:20:00\n",
            "Index(['Unnamed: 0', 'Relative Humidity', 'Temperature', 'hour',\n",
            "       'datetime_america'],\n",
            "      dtype='object')\n",
            "(328242, 5)\n"
          ]
        }
      ],
      "source": [
        "#################\n",
        "# Preprocess inbound csv:\n",
        "#################\n",
        "print('\\n #####\\nInbound: \\n ##### \\n')\n",
        "inbound_post = inbound[inbound.carrier_code != 'CANCEL']\n",
        "inbound_post = inbound[inbound.carrier_code != '']\n",
        "\n",
        "inbound_post['truck_signin_datetime'] = pd.to_datetime(inbound_post['truck_signin_datetime'])\n",
        "\n",
        "# Compute delta times\n",
        "inbound_load_time = pd.to_datetime(inbound_post['load_finish_datetime']) - pd.to_datetime(inbound_post['load_start_datetime'])\n",
        "inbound_truck_time = pd.to_datetime(inbound_post['truck_signin_datetime']) - pd.to_datetime(inbound_post['signout_datetime'])\n",
        "\n",
        "# Drop unnecessary columns\n",
        "inbound_post = inbound_post.drop(['Unnamed: 0', 'warehouse_order_number', 'customer_code', 'load_reference_number', 'carrier_code', 'weight_uom', 'load_finish_datetime', 'load_start_datetime', 'dock_door_number', 'trailer_number', 'signout_datetime'], axis=1)\n",
        "\n",
        "# Add time deltas\n",
        "inbound_post['load_time'] = inbound_load_time\n",
        "inbound_post['truck_time'] = inbound_truck_time\n",
        "\n",
        "print(inbound_post.columns)\n",
        "\n",
        "inbound_post['load_time'] = inbound_post['load_time'].dt.seconds\n",
        "inbound_post['truck_time'] = inbound_post['truck_time'].dt.seconds\n",
        "\n",
        "print('With NaN:', inbound_post.shape)\n",
        "\n",
        "# Drop rows with >0 NaN values\n",
        "inbound_post_nan = inbound_post.dropna().reset_index(drop=True)\n",
        "\n",
        "print('Without NaN:', inbound_post_nan.shape)\n",
        "\n",
        "#################\n",
        "# Preprocess outbound csv:\n",
        "#################\n",
        "print('\\n #####\\nOutbound: \\n ##### \\n')\n",
        "outbound_post = outbound[outbound.carrier_code != 'CANCEL']\n",
        "outbound_post = outbound[outbound.carrier_code != 'VOID']\n",
        "outbound_post = outbound[outbound.carrier_code != '']\n",
        "\n",
        "outbound_post['truck_signin_datetime'] = pd.to_datetime(outbound_post['truck_signin_datetime'])\n",
        "\n",
        "# Compute delta times\n",
        "outbound_load_time = pd.to_datetime(outbound_post['load_finish_datetime']) - pd.to_datetime(outbound_post['load_start_datetime'])\n",
        "outbound_truck_time = pd.to_datetime(outbound_post['truck_signin_datetime']) - pd.to_datetime(outbound_post['signout_datetime'])\n",
        "\n",
        "# Drop unnecessary columns\n",
        "outbound_post = outbound_post.drop(['Unnamed: 0', 'warehouse_order_number', 'customer_code', 'load_reference_number', 'carrier_code', 'weight_uom', 'load_finish_datetime', 'load_start_datetime', 'dock_door_number', 'trailer_number', 'signout_datetime'], axis=1)\n",
        "\n",
        "# Add time deltas\n",
        "outbound_post['load_time'] = outbound_load_time\n",
        "outbound_post['truck_time'] = outbound_truck_time\n",
        "\n",
        "print(outbound_post.columns)\n",
        "\n",
        "outbound_post['load_time'] = outbound_post['load_time'].dt.seconds\n",
        "outbound_post['truck_time'] = outbound_post['truck_time'].dt.seconds\n",
        "\n",
        "print('With NaN:', outbound_post.shape)\n",
        "\n",
        "# Drop rows with >0 NaN values\n",
        "outbound_post_nan = outbound_post.dropna().reset_index(drop=True)\n",
        "\n",
        "print('Without NaN:', outbound_post_nan.shape)\n",
        "\n",
        "#################\n",
        "# Preprocess demand csv:\n",
        "#################\n",
        "print('\\n #####\\nDemand:  \\n ##### \\n')\n",
        "\n",
        "demand['datetime_local'] = pd.to_datetime(demand['datetime_local'])\n",
        "\n",
        "end_known_idx = demand[demand.demand_kW > 1].index[-1]\n",
        "train_val_split = 0.7 # 70% train, 30% val\n",
        "end_train_idx = int((train_val_split) * end_known_idx)\n",
        "demand_train = demand[0:end_train_idx-1]\n",
        "demand_val = demand[end_train_idx:end_known_idx]\n",
        "\n",
        "print('Full dataset:', demand.shape)\n",
        "print('Answers known until index: ', end_known_idx)\n",
        "print(f'Training set, {int(train_val_split*100)}%:', demand_train.shape)\n",
        "print(f'Validation set, {int(100-train_val_split*100)}%:', demand_val.shape)\n",
        "\n",
        "#################\n",
        "# Preprocess weather csv:\n",
        "#################\n",
        "print('\\n #####\\nWeather:  \\n ##### \\n')\n",
        "\n",
        "weather_post = weather.copy()\n",
        "UTC6 = pd.to_datetime(weather_post['datetime_UTC']) - pd.Timedelta(hours=6)\n",
        "weather_post['datetime_america'] = UTC6\n",
        "weather_post = weather_post.drop('datetime_UTC', axis=1)\n",
        "weather_post = weather_post.drop('datetime', axis=1)\n",
        "\n",
        "print(weather_post.head())\n",
        "print(weather_post.columns)\n",
        "print(weather_post.shape)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Merging csv files"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inbound with demand:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Unnamed: 0  datetime_local    demand_kW  front_temperature  \\\n",
            "0           0    1.546287e+09  2064.101392               -4.0   \n",
            "1           1    1.546288e+09  1874.002081               -4.0   \n",
            "2           2    1.546289e+09  1988.168511               -4.0   \n",
            "3           3    1.546290e+09  2022.795943               -4.0   \n",
            "4           4    1.546291e+09  1986.981872               -4.0   \n",
            "\n",
            "   middle_temperature  back_temperature  net_weight  case_quantity  \\\n",
            "0                -4.0              -4.0     45264.0         1476.0   \n",
            "1                -4.0              -4.0     45264.0         1476.0   \n",
            "2                -4.0              -4.0     45264.0         1476.0   \n",
            "3                -4.0              -4.0     45264.0         1476.0   \n",
            "4                -4.0              -4.0     45264.0         1476.0   \n",
            "\n",
            "   pallet_count  truck_signin_datetime  load_time  truck_time  \n",
            "0          24.0           1.546264e+09     1140.0     86399.0  \n",
            "1          24.0           1.546264e+09     1140.0     86399.0  \n",
            "2          24.0           1.546264e+09     1140.0     86399.0  \n",
            "3          24.0           1.546264e+09     1140.0     86399.0  \n",
            "4          24.0           1.546264e+09     1140.0     86399.0  \n"
          ]
        }
      ],
      "source": [
        "inbound_post_nan.sort_values(\"truck_signin_datetime\", inplace=True)\n",
        "demand_inbound_merge = pd.merge_asof(demand_train, inbound_post_nan, left_on='datetime_local', right_on='truck_signin_datetime', direction='nearest')\n",
        "\n",
        "demand_inbound_merge_numerical = demand_inbound_merge.copy()\n",
        "demand_inbound_merge_numerical['datetime_local'] = demand_inbound_merge_numerical['datetime_local'].apply(lambda x: time.mktime(x.timetuple()))\n",
        "demand_inbound_merge_numerical['truck_signin_datetime'] = demand_inbound_merge_numerical['truck_signin_datetime'].apply(lambda x: time.mktime(x.timetuple()))\n",
        "\n",
        "print(demand_inbound_merge_numerical.head())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Regression on train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8wQBLpyu9Zw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 135799.36623040738\n"
          ]
        }
      ],
      "source": [
        "# Split the data into training and test sets\n",
        "X = demand_inbound_merge_numerical.drop('demand_kW', axis=1)\n",
        "y = demand_inbound_merge_numerical['demand_kW']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', AdaBoostRegressor())\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model using mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print('Mean Squared Error:', mse)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding outbound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        Unnamed: 0  datetime_local  demand_kW  front_temperature  \\\n",
            "191784      191784    1.628928e+09   2561.692               41.0   \n",
            "191785      191785    1.628928e+09   2864.648               41.0   \n",
            "191786      191786    1.628928e+09   2820.785               41.0   \n",
            "191787      191787    1.628928e+09   2817.942               41.0   \n",
            "191788      191788    1.628928e+09   2699.152               41.0   \n",
            "\n",
            "        middle_temperature  back_temperature  net_weight_inbound  \\\n",
            "191784                42.0              42.0             41198.0   \n",
            "191785                42.0              42.0             41198.0   \n",
            "191786                42.0              42.0             41198.0   \n",
            "191787                42.0              42.0             41198.0   \n",
            "191788                42.0              42.0             41198.0   \n",
            "\n",
            "        case_quantity_inbound  pallet_count_inbound  \\\n",
            "191784                 1642.0                  28.0   \n",
            "191785                 1642.0                  28.0   \n",
            "191786                 1642.0                  28.0   \n",
            "191787                 1642.0                  28.0   \n",
            "191788                 1642.0                  28.0   \n",
            "\n",
            "        truck_signin_datetime_inbound  load_time_inbound  truck_time_inbound  \\\n",
            "191784                   1.562101e+09             1020.0             86397.0   \n",
            "191785                   1.562101e+09             1020.0             86397.0   \n",
            "191786                   1.562101e+09             1020.0             86397.0   \n",
            "191787                   1.562101e+09             1020.0             86397.0   \n",
            "191788                   1.562101e+09             1020.0             86397.0   \n",
            "\n",
            "        net_weight_outbound  case_quantity_outbound  pallet_count_outbound  \\\n",
            "191784               9595.0                   497.0                    7.0   \n",
            "191785               9595.0                   497.0                    7.0   \n",
            "191786               9595.0                   497.0                    7.0   \n",
            "191787               9595.0                   497.0                    7.0   \n",
            "191788               9595.0                   497.0                    7.0   \n",
            "\n",
            "        truck_signin_datetime_outbound  load_time_outbound  \\\n",
            "191784                    1.628928e+09              4081.0   \n",
            "191785                    1.628928e+09              4081.0   \n",
            "191786                    1.628928e+09              4081.0   \n",
            "191787                    1.628928e+09              4081.0   \n",
            "191788                    1.628928e+09              4081.0   \n",
            "\n",
            "        truck_time_outbound  \n",
            "191784              71137.0  \n",
            "191785              71137.0  \n",
            "191786              71137.0  \n",
            "191787              71137.0  \n",
            "191788              71137.0  \n"
          ]
        }
      ],
      "source": [
        "outbound_post_nan.sort_values(\"truck_signin_datetime\", inplace=True)\n",
        "outbound_merge = pd.merge_asof(demand_inbound_merge, outbound_post_nan, left_on='datetime_local', right_on='truck_signin_datetime', direction='nearest', suffixes=('_inbound', '_outbound'))\n",
        "\n",
        "outbound_merge_numerical = outbound_merge.copy()\n",
        "outbound_merge_numerical['datetime_local'] = outbound_merge_numerical['datetime_local'].apply(lambda x: time.mktime(x.timetuple()))\n",
        "outbound_merge_numerical['truck_signin_datetime_inbound'] = outbound_merge_numerical['truck_signin_datetime_inbound'].apply(lambda x: time.mktime(x.timetuple()))\n",
        "outbound_merge_numerical['truck_signin_datetime_outbound'] = outbound_merge_numerical['truck_signin_datetime_outbound'].apply(lambda x: time.mktime(x.timetuple()))\n",
        "\n",
        "print(outbound_merge_numerical.tail())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Regression on train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 181756.84203538956\n"
          ]
        }
      ],
      "source": [
        "# Split the data into training and test sets\n",
        "X = outbound_merge_numerical.drop('demand_kW', axis=1)\n",
        "y = outbound_merge_numerical['demand_kW']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up pipeline parameters\n",
        "params = {\n",
        "    \"n_estimators\": 50,\n",
        "    \"learning_rate\": 1,\n",
        "    \"loss\": \"square\",\n",
        "}\n",
        "\n",
        "# Set up the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', AdaBoostRegressor(**params))\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model using mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print('Mean Squared Error:', mse)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding weather"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(191789, 23)\n"
          ]
        }
      ],
      "source": [
        "weather_post.sort_values(\"datetime_america\", inplace=True)\n",
        "weather_merge = pd.merge_asof(outbound_merge, weather_post, left_on='datetime_local', right_on='datetime_america', direction='nearest')\n",
        "\n",
        "weather_merge_numerical = weather_merge.copy()\n",
        "weather_merge_numerical['datetime_local'] = weather_merge_numerical['datetime_local'].apply(lambda x: time.mktime(x.timetuple()))\n",
        "weather_merge_numerical['truck_signin_datetime_inbound'] = weather_merge_numerical['truck_signin_datetime_inbound'].apply(lambda x: time.mktime(x.timetuple()))\n",
        "weather_merge_numerical['truck_signin_datetime_outbound'] = weather_merge_numerical['truck_signin_datetime_outbound'].apply(lambda x: time.mktime(x.timetuple()))\n",
        "weather_merge_numerical['datetime_america'] = weather_merge_numerical['datetime_america'].apply(lambda x: time.mktime(x.timetuple()))\n",
        "\n",
        "print(weather_merge_numerical.shape)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Regression on train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 122017.98412082123\n"
          ]
        }
      ],
      "source": [
        "# Split the data into training and test sets\n",
        "X = weather_merge_numerical.drop('demand_kW', axis=1)\n",
        "y = weather_merge_numerical['demand_kW']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Needed again bcause of NaN values\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', AdaBoostRegressor())\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model using mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print('Mean Squared Error:', mse)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Removing the date columns, because their magnitude may be interfering with the regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Unnamed: 0_x', 'demand_kW', 'front_temperature', 'middle_temperature',\n",
            "       'back_temperature', 'net_weight_inbound', 'case_quantity_inbound',\n",
            "       'pallet_count_inbound', 'load_time_inbound', 'truck_time_inbound',\n",
            "       'net_weight_outbound', 'case_quantity_outbound',\n",
            "       'pallet_count_outbound', 'load_time_outbound', 'truck_time_outbound',\n",
            "       'Unnamed: 0_y', 'Relative Humidity', 'Temperature', 'hour'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "no_dates_merged = weather_merge_numerical.drop(['datetime_local', 'truck_signin_datetime_inbound', 'truck_signin_datetime_outbound', 'datetime_america'], axis=1)\n",
        "print(no_dates_merged.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;imputer&#x27;, SimpleImputer()), (&#x27;scaler&#x27;, StandardScaler()),\n",
              "                (&#x27;regressor&#x27;,\n",
              "                 GradientBoostingRegressor(learning_rate=1, n_estimators=50))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;imputer&#x27;, SimpleImputer()), (&#x27;scaler&#x27;, StandardScaler()),\n",
              "                (&#x27;regressor&#x27;,\n",
              "                 GradientBoostingRegressor(learning_rate=1, n_estimators=50))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=1, n_estimators=50)</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler()),\n",
              "                ('regressor',\n",
              "                 GradientBoostingRegressor(learning_rate=1, n_estimators=50))])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Split the data into training and test sets\n",
        "X = no_dates_merged.drop('demand_kW', axis=1)\n",
        "y = no_dates_merged['demand_kW']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up pipeline parameters\n",
        "params = {\n",
        "    \"n_estimators\": 50,\n",
        "    \"learning_rate\": 1,\n",
        "    \"loss\": \"squared_error\",\n",
        "}\n",
        "# Set up the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')), # Needed again bcause of NaN values\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', ensemble.GradientBoostingRegressor(**params))\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "# y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model using mean squared error\n",
        "# mse = mean_squared_error(y_test, y_pred)\n",
        "# print('Mean Squared Error:', mse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Pipeline' object has no attribute 'regressor'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\karan\\Desktop\\energy-demand-prediction\\analysis.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/karan/Desktop/energy-demand-prediction/analysis.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/karan/Desktop/energy-demand-prediction/analysis.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mDeviance\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/karan/Desktop/energy-demand-prediction/analysis.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/karan/Desktop/energy-demand-prediction/analysis.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     np\u001b[39m.\u001b[39marange(params[\u001b[39m\"\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/karan/Desktop/energy-demand-prediction/analysis.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     pipeline\u001b[39m.\u001b[39;49mregressor\u001b[39m.\u001b[39mtrain_score_,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/karan/Desktop/energy-demand-prediction/analysis.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mb-\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/karan/Desktop/energy-demand-prediction/analysis.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining Set Deviance\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/karan/Desktop/energy-demand-prediction/analysis.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/karan/Desktop/energy-demand-prediction/analysis.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/karan/Desktop/energy-demand-prediction/analysis.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     np\u001b[39m.\u001b[39marange(params[\u001b[39m\"\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, test_score, \u001b[39m\"\u001b[39m\u001b[39mr-\u001b[39m\u001b[39m\"\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest Set Deviance\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/karan/Desktop/energy-demand-prediction/analysis.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/karan/Desktop/energy-demand-prediction/analysis.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m plt\u001b[39m.\u001b[39mlegend(loc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mupper right\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'regressor'"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "y_pred = pipeline.predict(X_test)\n",
        "test_score = mean_squared_error(y_pred, y_test)\n",
        "\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.title(\"Deviance\")\n",
        "plt.plot(\n",
        "    np.arange(params[\"n_estimators\"]) + 1,\n",
        "    pipeline.regressor.train_score_,\n",
        "    \"b-\",\n",
        "    label=\"Training Set Deviance\",\n",
        ")\n",
        "plt.plot(\n",
        "    np.arange(params[\"n_estimators\"]) + 1, test_score, \"r-\", label=\"Test Set Deviance\"\n",
        ")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.xlabel(\"Boosting Iterations\")\n",
        "plt.ylabel(\"Deviance\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "a43632599ce3a37c0054abbf953200660aecbe64e6d247a294a3afed41a103e3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
